name: Scrape OSM

on:
  workflow_dispatch:

jobs:
  scrape-osm:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Build Docker image
        run: docker build -t osm-scraper .

      - name: Run scraper
        run: docker run -v ${{ github.workspace }}/data:/app/data osm-scraper

      - name: Create timestamp variable
        id: ts
        run: echo "ts=$(date +'%Y-%m-%d')" >> $GITHUB_OUTPUT

      # - name: Upload scraped data as artifact
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: country-data-${{ github.run_number }}_${{ steps.ts.outputs.ts }}
      #     path: data/raw
      #     retention-days: 30

      - name: Install AWS CLI
        run: sudo apt-get update && sudo apt-get install -y awscli

      - name: Upload files to Backblaze B2 (S3 compatible)
        env:
            AWS_ACCESS_KEY_ID: ${{ secrets.B2_KEY_ID }}
            AWS_SECRET_ACCESS_KEY: ${{ secrets.B2_APPLICATION_KEY }}
            AWS_DEFAULT_REGION: us-east-005
        run: |
          ts="${{ steps.ts.outputs.ts }}"
          aws s3 cp data/raw \
            s3://${{ secrets.B2_BUCKET_NAME }}/runs/${{ github.run_number }}_${ts}/ \
            --recursive \
            --endpoint-url "${{ secrets.B2_ENDPOINT }}"